# Participants à la session février 2023

Ce fichier permet de lister tous les participants de la session, ainsi que le détail de leur proposition sur le sujet indiqué.

*Merci de remplir ce fichier avec le plus grand soin, en respectant la structure du tableau*.

| **Nom du participant(s)** | **Titre de la proposition** | **Description de la proposition** | **Lien Github** | **Status**  |
|---------------------------|-----------------------------|-----------------------------------|-----------------|-------------|
| Elvis MBONING             | Optimisation et augmentation d'entité par position pour la détection d'entités nommées dans une langue peu dotée: cas du Ghomala | Nous proposons une solution alternative beaucoup plus simple, rapide et efficace utilisant les champs aléatoires conditionnels (CRFs) avec une optimisation des descripteurs linguistiques propres à ces langues. Nous verrons aussi comment l'utilisation d'une nouvelle approche simplifiée de l'augmentation que nous avons appelé " position to position entity augmentation "  améliore substantiellement les résultats et surpasse l'état de l'art actuel dans le domaine en Afrique. | [SCIA-CRF_LF](https://github.com/Levis0045/SCIA-CRF_LF) | Complete |
| Tatiana Moteu             | Simple algorithmes de ML    | Perceptron, NB, SGD, Passive Aggressive classifier  |  | In progress |
| Moussa Aboubakar          |                             | ...                              | [SCIA-MTO](https://github.com/abakamousa/NER-Sangkak-challenge)                  | In progress            |
| Rikel Anderson            |  Transformer with data augmentation                           | Transformers and BERT Model: Transformers are a particular architecture for deep learning models that revolutionized natural language processing. The defining characteristic for a Transformer is the self-attention mechanism. Tranformer approach has been proven to be the most performing solution for language and NLP task. In this approah we will fine tune Transformer model(BERT) with a our low resource language with some data augmetation in order to increase the low resource training size.                                  |   https://github.com/leriky/SCIA-TRANSF              | Complete            |

